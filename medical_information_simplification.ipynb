{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplification & Summarization of Medical Information for Elderly Patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project for CS4120 Natural Language Processing (Fall 2024).\n",
    "\n",
    "Contributors: Lucas Dunker, Shashank Jarmale, Andrew Sun, Dylan Weinmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Information & Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Describe the background information for why this model is useful, particularly for elderly patients\n",
    "- Describe the datasets we are using and which models we are evaluating\n",
    "- Give instructions for how to run our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import PreTrainedTokenizer, T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print(f\"Using device {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikismall Dataset for General Text Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikismall Dataset Constants\n",
    "\n",
    "WIKISMALL_TRAIN_SOURCE_PATH = \"datasets/wikismall/train_source.txt\"\n",
    "WIKISMALL_TRAIN_TARGET_PATH = \"datasets/wikismall/train_target.txt\"\n",
    "WIKISMALL_VALIDATION_SOURCE_PATH = \"datasets/wikismall/val_source.txt\"\n",
    "WIKISMALL_VALIDATION_TARGET_PATH = \"datasets/wikismall/val_target.txt\"\n",
    "WIKISMALL_TEST_SOURCE_PATH = \"datasets/wikismall/test_source.txt\"\n",
    "WIKISMALL_TEST_TARGET_PATH = \"datasets/wikismall/test_target.txt\"\n",
    "\n",
    "# TODO: might want to adjust both of these\n",
    "WIKISMALL_BATCH_SIZE = 16\n",
    "WIKISMALL_MAX_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikismallDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class representation for loading in the Wikismall dataset, allowing for fine-tuning of pretrained models with general text simplification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source_file: str, target_file: str, tokenizer: PreTrainedTokenizer, max_length: int):\n",
    "        \"\"\"\n",
    "        Initializes the Wikismall dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - source_file (str): Path to text source file.\n",
    "        - target_file (str): Path to text target file (simplified information).\n",
    "        - tokenizer (PreTrainedTokenizer): The tokenizer to use for encoding.\n",
    "        - max_length (int): The maximum length to use for tokenization.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(source_file, \"r\") as f:\n",
    "            source_lines = [line.strip() for line in f]\n",
    "        \n",
    "        with open(target_file, \"r\") as f:\n",
    "            target_lines = [line.strip() for line in f]\n",
    "        \n",
    "        assert len(source_lines) == len(target_lines), \"Source and target dataset files must have same number of lines\"\n",
    "\n",
    "        # Tokenize the data and save it for later\n",
    "        self.data = []\n",
    "        for source_line, target_line in zip(source_lines, target_lines):\n",
    "            source_tokenized = tokenizer(source_line, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            target_tokenized = tokenizer(target_line, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            self.data.append({\n",
    "                \"input_ids\": source_tokenized[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": source_tokenized[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": target_tokenized[\"input_ids\"].squeeze(),\n",
    "            })\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Calculates the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - (int): The number of samples in the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Dict[str, dict]:\n",
    "        \"\"\"\n",
    "        For a given index in the dataset, retrieves the source and target encodings.\n",
    "\n",
    "        Parameters:\n",
    "        - i (int): The index of the relevant sample in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - (Dict[str, dict]): The input IDs, attention mask, and labels for the data sample.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikismall_dataset(tokenizer: PreTrainedTokenizer, batch_size: int) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads the Wikismall dataset into three different PyTorch DataLoaders, for training, validation, and test.\n",
    "\n",
    "    Parameters:\n",
    "    - tokenizer (PreTrainedTokenizer): The tokenizer to use for encoding.\n",
    "    - batch_size (int): The batch size to use for data loading.\n",
    "\n",
    "    Returns:\n",
    "    - (Tuple[DataLoader, DataLoader, DataLoader]): The DataLoaders for the train, validation, and test datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    train_dataset = WikismallDataset(\n",
    "        WIKISMALL_TRAIN_SOURCE_PATH,\n",
    "        WIKISMALL_TRAIN_TARGET_PATH,\n",
    "        tokenizer,\n",
    "        WIKISMALL_MAX_LENGTH,\n",
    "    )\n",
    "    val_dataset = WikismallDataset(\n",
    "        WIKISMALL_VALIDATION_SOURCE_PATH,\n",
    "        WIKISMALL_VALIDATION_TARGET_PATH,\n",
    "        tokenizer,\n",
    "        WIKISMALL_MAX_LENGTH,\n",
    "    )\n",
    "    test_dataset = WikismallDataset(\n",
    "        WIKISMALL_TEST_SOURCE_PATH,\n",
    "        WIKISMALL_TEST_TARGET_PATH,\n",
    "        tokenizer,\n",
    "        WIKISMALL_MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC-IV-Ext-BHC Dataset for Medical Information Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIMIC-IV-Ext-BHC Dataset Constants\n",
    "\n",
    "MIMIC_DATASET_CSV_PATH = \"datasets/mimic-iv-ext-bhc/mimic-iv-bhc.csv\"\n",
    "\n",
    "# TODO: this dataset is massive. we might not want to use all of it, to keep training times within reason.\n",
    "MIMIC_DATA_USAGE_FRACTION = 0.25\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "MIMIC_TEST_SIZE = 0.2\n",
    "MIMIC_VAL_SIZE = 0.25  # Proportion of training data used for validation\n",
    "\n",
    "# TODO: might want to adjust both of these\n",
    "MIMIC_BATCH_SIZE = 16\n",
    "MIMIC_MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIMICDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class representation for loading in the MIMIC dataset, allowing for fine-tuning of pretrained models with text simplification\n",
    "    specific to medical information/patient summaries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: PreTrainedTokenizer, max_length: int):\n",
    "        \"\"\"\n",
    "        Initializes the Wikismall dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): The pandas dataframe for this dataset.\n",
    "        - tokenizer (PreTrainedTokenizer): The tokenizer to use for encoding.\n",
    "        - max_length (int): The maximum length to use for tokenization.\n",
    "        \"\"\"\n",
    "\n",
    "        # Tokenize the data and save it for later\n",
    "        self.df = df\n",
    "        self.df[\"input_ids\"] = self.df[\"input\"].apply(lambda x: tokenizer(x, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"].squeeze())\n",
    "        self.df[\"attention_mask\"] = self.df[\"input\"].apply(lambda x: tokenizer(x, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"attention_mask\"].squeeze())\n",
    "        self.df[\"labels\"] = self.df[\"target\"].apply(lambda x: tokenizer(x, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"].squeeze())\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Calculates the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - (int): The number of samples in the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Dict[str, dict]:\n",
    "        \"\"\"\n",
    "        For a given index in the dataset, retrieves the source and target encodings.\n",
    "\n",
    "        Parameters:\n",
    "        - i (int): The index of the relevant sample in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - (Dict[str, dict]): The input IDs, attention mask, and labels for the data sample.\n",
    "        \"\"\"\n",
    "\n",
    "        row = self.df.iloc[i]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": row[\"input_ids\"],\n",
    "            \"attention_mask\": row[\"attention_mask\"],\n",
    "            \"labels\": row[\"labels\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mimic_dataset(mimic_df: pd.DataFrame, tokenizer: PreTrainedTokenizer, batch_size: int) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads the MIMIC-IV-Ext-BHC dataset into three different PyTorch DataLoaders, for training, validation, and test.\n",
    "\n",
    "    Parameters:\n",
    "    - mimic_df (pd.DataFrame): The entire MIMIC dataset as a pandas dataframe.\n",
    "    - tokenizer (PreTrainedTokenizer): The tokenizer to use for encoding.\n",
    "    - batch_size (int): The batch size to use for data loading.\n",
    "\n",
    "    Returns:\n",
    "    - (Tuple[DataLoader, DataLoader, DataLoader]): The DataLoaders for the train, validation, and test datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample from the dataframe to only use however much data we want\n",
    "    mimic_df = mimic_df.sample(frac=MIMIC_DATA_USAGE_FRACTION)\n",
    "\n",
    "    # Split overall dataframe into train, val, and test dataframes\n",
    "    train_val_df, test_df = train_test_split(mimic_df, test_size=MIMIC_TEST_SIZE, random_state=RANDOM_STATE)\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=MIMIC_VAL_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "    train_dataset = MIMICDataset(\n",
    "        train_df,\n",
    "        tokenizer,\n",
    "        MIMIC_MAX_LENGTH,\n",
    "    )\n",
    "    val_dataset = MIMICDataset(\n",
    "        val_df,\n",
    "        tokenizer,\n",
    "        MIMIC_MAX_LENGTH,\n",
    "    )\n",
    "    test_dataset = MIMICDataset(\n",
    "        test_df,\n",
    "        tokenizer,\n",
    "        MIMIC_MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_df = pd.read_csv(MIMIC_DATASET_CSV_PATH)\n",
    "mimic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Pretrained Models on the Wikismall Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in base T5 model and tokenizer\n",
    "\n",
    "# TODO: could use \"t5-small\" if this takes too long to train\n",
    "t5_model_name = \"t5-base\"\n",
    "\n",
    "t5_base = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "t5_base.to(DEVICE)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "\n",
    "# Freeze base model parameters\n",
    "for param in t5_base.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Wikismall data loaders for T5 training, validation, and test\n",
    "\n",
    "t5_wikismall_train_loader, t5_wikismall_val_loader, t5_wikismall_test_loader = load_wikismall_dataset(t5_tokenizer, WIKISMALL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fine-tunable T5 model (base T5 with an additional layer processing the output)\n",
    "\n",
    "class T5Simplifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a T5 model fine-tuned on the Wikismall dataset in order to do text simplification as a sequence generation model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, t5_base):\n",
    "        \"\"\"\n",
    "        Initializes the PyTorch model for fine-tuning T5.\n",
    "\n",
    "        Parameters:\n",
    "        - t5_base (T5ForConditionalGeneration): The T5 base model.\n",
    "        \"\"\"\n",
    "\n",
    "        super(T5Simplifier, self).__init__()\n",
    "        self.t5_base = t5_base\n",
    "        self.layer1 = nn.Linear(t5_base.config.d_model, t5_base.config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \"\"\"\n",
    "        Runs a forward pass for the T5 model fine-tuned to do text simplification/summarization.\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.t5_base(input_ids=input_ids, attention_mask=attention_mask, labels=labels, output_hidden_states=True)\n",
    "        hidden_states = outputs.encoder_last_hidden_state\n",
    "        logits = self.layer1(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the fine-tunable T5 model & optimizer\n",
    "\n",
    "t5_simplifier = T5Simplifier(t5_base).to(DEVICE)\n",
    "\n",
    "t5_optimizer = AdamW(t5_simplifier.layer1.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (fine-tune the T5 model on the Wikismall dataset)\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    t5_simplifier.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(t5_wikismall_train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        t5_optimizer.zero_grad()\n",
    "\n",
    "        # Move pre-tokenized inputs and labels to the device\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        loss, _ = t5_simplifier(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        t5_optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} Loss: {total_loss / len(t5_wikismall_train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    t5_simplifier.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch in t5_wikismall_val_loader:\n",
    "            # Move pre-tokenized inputs and labels to the device\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            loss, _ = t5_simplifier(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss / len(t5_wikismall_val_loader)}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "t5_simplifier.save_pretrained(\"t5-wikismall-finetuned\")\n",
    "t5_tokenizer.save_pretrained(\"t5-wikismall-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Pretrained Models on the MIMIC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in base T5 model and tokenizer\n",
    "\n",
    "# TODO: we should eventually be fine-tuning the version of T5 already fine-tuned on Wikismall here\n",
    "\n",
    "t5_model_name = \"t5-base\"\n",
    "\n",
    "t5_base = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "t5_base.to(DEVICE)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "\n",
    "# Freeze base model parameters\n",
    "for param in t5_base.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MIMIC data loaders for T5 training, validation, and test\n",
    "\n",
    "# TODO: took 8 minutes to run with MIMIC_DATA_USAGE_FRACTION set to 0.25 - probably worth saving the final dataframes and just loading/using those directly\n",
    "t5_mimic_train_loader, t5_mimic_val_loader, t5_mimic_test_loader = load_mimic_dataset(mimic_df, t5_tokenizer, MIMIC_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigBirdPegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
