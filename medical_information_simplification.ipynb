{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplification & Summarization of Medical Information for Elderly Patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Information & Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Describe the background information for why this model is useful, particularly for elderly patients\n",
    "- Describe the datasets we are using and which models we are evaluating\n",
    "- Give instructions for how to run our code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikismall Dataset for General Text Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikismall Dataset Constants\n",
    "\n",
    "WIKISMALL_TRAIN_SOURCE_PATH = \"datasets/wikismall/train_source.txt\"\n",
    "WIKISMALL_TRAIN_TARGET_PATH = \"datasets/wikismall/train_target.txt\"\n",
    "WIKISMALL_VALIDATION_SOURCE_PATH = \"datasets/wikismall/val_source.txt\"\n",
    "WIKISMALL_VALIDATION_TARGET_PATH = \"datasets/wikismall/val_target.txt\"\n",
    "WIKISMALL_TEST_SOURCE_PATH = \"datasets/wikismall/test_source.txt\"\n",
    "WIKISMALL_TEST_TARGET_PATH = \"datasets/wikismall/test_target.txt\"\n",
    "\n",
    "WIKISMALL_BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikismallDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class representation for loading in the Wikismall dataset, allowing for fine-tuning of pretrained models with general text simplification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source_file: str, target_file: str, tokenizer: PreTrainedTokenizer):\n",
    "        \"\"\"\n",
    "        Initializes the Wikismall dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - source_file (str): Path to text source file.\n",
    "        - target_file (str): Path to text target file (simplified information).\n",
    "        - tokenizer (PreTrainedTokenizer): The tokenizer to use for encoding.\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        with open(source_file, \"r\") as f:\n",
    "            self.source_lines = [line.strip() for line in f]\n",
    "        \n",
    "        with open(target_file, \"r\") as f:\n",
    "            self.target_lines = [line.strip() for line in f]\n",
    "        \n",
    "        assert len(self.source_lines) == len(self.target_lines), \"Source and target dataset files must have same number of lines\"\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Calculates the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - (int): The number of samples in the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.source_lines)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        For a given index in the dataset, retrieves the source and target encodings.\n",
    "\n",
    "        Parameters:\n",
    "        - i (int): The index of the relevant sample in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - (Dict[str, torch.Tensor]): The mapping of source and target encodings for the data sample.\n",
    "        \"\"\"\n",
    "\n",
    "        source_line = self.source_lines[i]\n",
    "        target_line = self.target_lines[i]\n",
    "\n",
    "        source_encodings = self.tokenizer(\n",
    "            source_line,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target_encodings = self.tokenizer(\n",
    "            target_line,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": source_encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": source_encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_encodings[\"input_ids\"].squeeze(),\n",
    "            \"decoder_attention_mask\": target_encodings[\"attention_mask\"].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikismall_dataset(tokenizer: PreTrainedTokenizer, batch_size: int) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads the Wikismall dataset into three different PyTorch DataLoaders, for training, validation, and test.\n",
    "\n",
    "    Parameters:\n",
    "    - tokenizer (PreTrainedTokenizer): The tokenizer to use for encoding.\n",
    "    - batch_size (int): The batch size to use for data loading.\n",
    "\n",
    "    Returns:\n",
    "    - (Tuple[DataLoader, DataLoader, DataLoader]): The DataLoaders for the train, validation, and test datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    train_dataset = WikismallDataset(\n",
    "        WIKISMALL_TRAIN_SOURCE_PATH,\n",
    "        WIKISMALL_TRAIN_TARGET_PATH,\n",
    "        tokenizer,\n",
    "    )\n",
    "    val_dataset = WikismallDataset(\n",
    "        WIKISMALL_VALIDATION_SOURCE_PATH,\n",
    "        WIKISMALL_VALIDATION_TARGET_PATH,\n",
    "        tokenizer,\n",
    "    )\n",
    "    test_dataset = WikismallDataset(\n",
    "        WIKISMALL_TEST_SOURCE_PATH,\n",
    "        WIKISMALL_TEST_TARGET_PATH,\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# TODO: need to use model-specific tokenizers for this\n",
    "train_loader, val_loader, test_loader = load_wikismall_dataset(None, WIKISMALL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIMIC-IV-Ext-BHC Dataset for Medical Information Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: need to load in this dataset once granted access to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Pretrained Models on the Wikismall Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
