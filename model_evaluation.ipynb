{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import Trainer, T5Tokenizer, T5ForConditionalGeneration, TrainingArguments\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    # Preprocess the text\n",
    "    inputs = tokenizer.encode(\n",
    "        \"summarize: \" + text,\n",
    "        return_tensors='pt',\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True\n",
    "    )\n",
    " \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "    )\n",
    " \n",
    "    # Decode and return the summary\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes is also known as diabetes mellitus. too much glucose stays in your blood\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"Diabetes, also known as diabetes mellitus, is a disease in which your blood glucose, or blood sugar, levels are too high. Glucose is your body's main source of energy. Your body can make glucose, but it also comes from the food you eat. Insulin is a hormone made by your pancreas. Insulin helps move glucose from your bloodstream into your cells, where it can be used for energy.If you have diabetes, your body can't make insulin, can't use insulin as well as it should, or both. Too much glucose stays in your blood and doesn't reach your cells. This can cause glucose levels to get too high. Over time, high blood glucose levels can lead to serious health conditions. But you can take steps to manage your diabetes and try to prevent these health problems.\"\n",
    "print(summarize_text(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target'],\n",
      "    num_rows: 88836\n",
      "})\n",
      "Dataset({\n",
      "    features: ['source', 'target'],\n",
      "    num_rows: 205\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "wikismall_train_df = pd.read_csv(\"datasets/wikismall/train.csv\")\n",
    "wikismall_val_df = pd.read_csv(\"datasets/wikismall/val.csv\")\n",
    "\n",
    "wikismall_train = Dataset.from_pandas(wikismall_train_df)\n",
    "wikismall_val = Dataset.from_pandas(wikismall_val_df)\n",
    "\n",
    "print(wikismall_train)\n",
    "print(wikismall_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['note_id', 'source', 'target', 'input_tokens', 'target_tokens', '__index_level_0__'],\n",
      "    num_rows: 17500\n",
      "})\n",
      "Dataset({\n",
      "    features: ['note_id', 'source', 'target', 'input_tokens', 'target_tokens', '__index_level_0__'],\n",
      "    num_rows: 3750\n",
      "})\n",
      "Dataset({\n",
      "    features: ['note_id', 'source', 'target', 'input_tokens', 'target_tokens', '__index_level_0__'],\n",
      "    num_rows: 3750\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "MIMIC_DATA_USAGE_ROWS = 25000\n",
    "MIMIC_VAL_SIZE = 0.3\n",
    "MIMIC_TEST_SIZE = 0.5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "mimic_df = pd.read_csv(\"datasets/mimic-iv-ext-bhc/mimic-iv-bhc.csv\", nrows=MIMIC_DATA_USAGE_ROWS)\n",
    "mimic_df.rename(columns={'input': 'source'}, inplace=True)\n",
    "\n",
    "mimic_train_df, mimic_rest_df = train_test_split(mimic_df, test_size=MIMIC_VAL_SIZE, random_state=RANDOM_STATE)\n",
    "mimic_val_df, mimic_test_df = train_test_split(mimic_rest_df, test_size=MIMIC_TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "mimic_train = Dataset.from_pandas(mimic_train_df)\n",
    "mimic_val = Dataset.from_pandas(mimic_val_df)\n",
    "mimic_test = Dataset.from_pandas(mimic_test_df)\n",
    "\n",
    "print(mimic_train)\n",
    "print(mimic_val)\n",
    "print(mimic_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611cbae891454975a1d37015281ebde6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88836 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1043f415a1c441dca7ca313703b2c3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH_WIKISMALL = 256\n",
    "def preprocess(dataset):\n",
    "    sources = [f\"summarize: {source}\" for source in dataset['source']]\n",
    "    tokens = tokenizer(sources, max_length=MAX_LENGTH_WIKISMALL, truncation=True, padding='max_length')\n",
    " \n",
    "    targets = [target for target in dataset['target']]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=MAX_LENGTH_WIKISMALL, truncation=True, padding='max_length')\n",
    " \n",
    "    tokens[\"labels\"] = labels[\"input_ids\"]\n",
    "    return tokens\n",
    " \n",
    "tokenized_train_wikismall = wikismall_train.map(preprocess, batched=True)\n",
    "tokenized_valid_wikismall = wikismall_val.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770abd1964cd440f8fb15411d4570518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4a830d87a246c3a59b9866fc20c69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cec2b2937ea4b7aa1a84d1bc254484f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH_MIMIC = 512\n",
    "def preprocess(dataset):\n",
    "    sources = [f\"summarize: {source}\" for source in dataset['source']]\n",
    "    tokens = tokenizer(sources, max_length=MAX_LENGTH_MIMIC, truncation=True, padding='max_length')\n",
    " \n",
    "    targets = [target for target in dataset['target']]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=MAX_LENGTH_MIMIC, truncation=True, padding='max_length')\n",
    " \n",
    "    tokens[\"labels\"] = labels[\"input_ids\"]\n",
    "    return tokens\n",
    "\n",
    "tokenized_mimic_train = mimic_train.map(preprocess, batched=True)\n",
    "tokenized_mimic_val = mimic_val.map(preprocess, batched=True)\n",
    "tokenized_mimic_test = mimic_test.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py\n",
    "\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    " \n",
    "def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        # Replace -100s used for padding as we can't decode them\n",
    "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory leak bug fix: \n",
    "# Source: https://discuss.huggingface.co/t/cuda-out-of-memory-when-using-trainer-with-compute-metrics/2941\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf617108644f402f88849d9f5fbec24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3325, 'learning_rate': 9.915574766986357e-05, 'epoch': 0.02}\n",
      "{'loss': 0.1157, 'learning_rate': 9.831149533972715e-05, 'epoch': 0.03}\n",
      "{'loss': 0.1171, 'learning_rate': 9.746724300959072e-05, 'epoch': 0.05}\n",
      "{'loss': 0.1132, 'learning_rate': 9.662299067945428e-05, 'epoch': 0.07}\n",
      "{'loss': 0.1094, 'learning_rate': 9.577873834931786e-05, 'epoch': 0.08}\n",
      "{'loss': 0.1104, 'learning_rate': 9.493448601918141e-05, 'epoch': 0.1}\n",
      "{'loss': 0.1121, 'learning_rate': 9.409023368904498e-05, 'epoch': 0.12}\n",
      "{'loss': 0.1096, 'learning_rate': 9.324598135890856e-05, 'epoch': 0.14}\n",
      "{'loss': 0.1111, 'learning_rate': 9.240172902877212e-05, 'epoch': 0.15}\n",
      "{'loss': 0.105, 'learning_rate': 9.155747669863569e-05, 'epoch': 0.17}\n",
      "{'loss': 0.1069, 'learning_rate': 9.071322436849927e-05, 'epoch': 0.19}\n",
      "{'loss': 0.1064, 'learning_rate': 8.986897203836283e-05, 'epoch': 0.2}\n",
      "{'loss': 0.108, 'learning_rate': 8.90247197082264e-05, 'epoch': 0.22}\n",
      "{'loss': 0.1049, 'learning_rate': 8.818046737808996e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1053, 'learning_rate': 8.733621504795354e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1071, 'learning_rate': 8.649196271781711e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1056, 'learning_rate': 8.564771038768068e-05, 'epoch': 0.29}\n",
      "{'loss': 0.1083, 'learning_rate': 8.480345805754424e-05, 'epoch': 0.3}\n",
      "{'loss': 0.1041, 'learning_rate': 8.39592057274078e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1054, 'learning_rate': 8.311495339727137e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1079, 'learning_rate': 8.227070106713495e-05, 'epoch': 0.35}\n",
      "{'loss': 0.1073, 'learning_rate': 8.142644873699852e-05, 'epoch': 0.37}\n",
      "{'loss': 0.105, 'learning_rate': 8.058219640686208e-05, 'epoch': 0.39}\n",
      "{'loss': 0.1026, 'learning_rate': 7.973794407672566e-05, 'epoch': 0.41}\n",
      "{'loss': 0.1056, 'learning_rate': 7.889369174658923e-05, 'epoch': 0.42}\n",
      "{'loss': 0.1029, 'learning_rate': 7.80494394164528e-05, 'epoch': 0.44}\n",
      "{'loss': 0.1033, 'learning_rate': 7.720518708631637e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1005, 'learning_rate': 7.636093475617994e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1019, 'learning_rate': 7.551668242604349e-05, 'epoch': 0.49}\n",
      "{'loss': 0.1028, 'learning_rate': 7.467243009590707e-05, 'epoch': 0.51}\n",
      "{'loss': 0.1035, 'learning_rate': 7.382817776577064e-05, 'epoch': 0.52}\n",
      "{'loss': 0.103, 'learning_rate': 7.29839254356342e-05, 'epoch': 0.54}\n",
      "{'loss': 0.1034, 'learning_rate': 7.213967310549778e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1097, 'learning_rate': 7.129542077536135e-05, 'epoch': 0.57}\n",
      "{'loss': 0.1036, 'learning_rate': 7.045116844522491e-05, 'epoch': 0.59}\n",
      "{'loss': 0.1036, 'learning_rate': 6.960691611508848e-05, 'epoch': 0.61}\n",
      "{'loss': 0.1064, 'learning_rate': 6.876266378495206e-05, 'epoch': 0.62}\n",
      "{'loss': 0.099, 'learning_rate': 6.791841145481562e-05, 'epoch': 0.64}\n",
      "{'loss': 0.1042, 'learning_rate': 6.707415912467919e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1045, 'learning_rate': 6.622990679454275e-05, 'epoch': 0.68}\n",
      "{'loss': 0.1063, 'learning_rate': 6.538565446440632e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1015, 'learning_rate': 6.454140213426988e-05, 'epoch': 0.71}\n",
      "{'loss': 0.1037, 'learning_rate': 6.369714980413346e-05, 'epoch': 0.73}\n",
      "{'loss': 0.1097, 'learning_rate': 6.285289747399703e-05, 'epoch': 0.74}\n",
      "{'loss': 0.1001, 'learning_rate': 6.20086451438606e-05, 'epoch': 0.76}\n",
      "{'loss': 0.1004, 'learning_rate': 6.116439281372417e-05, 'epoch': 0.78}\n",
      "{'loss': 0.1048, 'learning_rate': 6.032014048358774e-05, 'epoch': 0.79}\n",
      "{'loss': 0.1061, 'learning_rate': 5.9475888153451306e-05, 'epoch': 0.81}\n",
      "{'loss': 0.1009, 'learning_rate': 5.863163582331488e-05, 'epoch': 0.83}\n",
      "{'loss': 0.105, 'learning_rate': 5.778738349317845e-05, 'epoch': 0.84}\n",
      "{'loss': 0.1009, 'learning_rate': 5.694313116304201e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0956, 'learning_rate': 5.6098878832905575e-05, 'epoch': 0.88}\n",
      "{'loss': 0.1041, 'learning_rate': 5.525462650276915e-05, 'epoch': 0.89}\n",
      "{'loss': 0.1004, 'learning_rate': 5.441037417263271e-05, 'epoch': 0.91}\n",
      "{'loss': 0.1025, 'learning_rate': 5.3566121842496286e-05, 'epoch': 0.93}\n",
      "{'loss': 0.0996, 'learning_rate': 5.272186951235986e-05, 'epoch': 0.95}\n",
      "{'loss': 0.1041, 'learning_rate': 5.1877617182223424e-05, 'epoch': 0.96}\n",
      "{'loss': 0.1035, 'learning_rate': 5.1033364852086996e-05, 'epoch': 0.98}\n",
      "{'loss': 0.1034, 'learning_rate': 5.018911252195057e-05, 'epoch': 1.0}\n",
      "{'loss': 0.089, 'learning_rate': 4.934486019181413e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0855, 'learning_rate': 4.85006078616777e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0921, 'learning_rate': 4.765635553154127e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0918, 'learning_rate': 4.681210320140484e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0898, 'learning_rate': 4.596785087126841e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0911, 'learning_rate': 4.5123598541131976e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0894, 'learning_rate': 4.427934621099554e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0884, 'learning_rate': 4.3435093880859114e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0852, 'learning_rate': 4.259084155072268e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0894, 'learning_rate': 4.174658922058625e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0859, 'learning_rate': 4.0902336890449825e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0906, 'learning_rate': 4.0058084560313384e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0876, 'learning_rate': 3.9213832230176956e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0866, 'learning_rate': 3.836957990004053e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0904, 'learning_rate': 3.7525327569904094e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0903, 'learning_rate': 3.668107523976767e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0871, 'learning_rate': 3.583682290963123e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0865, 'learning_rate': 3.49925705794948e-05, 'epoch': 1.3}\n",
      "{'loss': 0.086, 'learning_rate': 3.414831824935837e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0903, 'learning_rate': 3.3304065919221936e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0885, 'learning_rate': 3.245981358908551e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0873, 'learning_rate': 3.161556125894908e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0912, 'learning_rate': 3.077130892881265e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0872, 'learning_rate': 2.9927056598676212e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0906, 'learning_rate': 2.908280426853978e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0862, 'learning_rate': 2.823855193840335e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0902, 'learning_rate': 2.7394299608266923e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0927, 'learning_rate': 2.6550047278130492e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0862, 'learning_rate': 2.5705794947994054e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0917, 'learning_rate': 2.4861542617857627e-05, 'epoch': 1.5}\n",
      "{'loss': 0.091, 'learning_rate': 2.4017290287721196e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0891, 'learning_rate': 2.3173037957584765e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0867, 'learning_rate': 2.232878562744833e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0901, 'learning_rate': 2.1484533297311903e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0874, 'learning_rate': 2.0640280967175472e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0884, 'learning_rate': 1.9796028637039038e-05, 'epoch': 1.6}\n",
      "{'loss': 0.087, 'learning_rate': 1.8951776306902607e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0884, 'learning_rate': 1.810752397676618e-05, 'epoch': 1.64}\n",
      "{'loss': 0.086, 'learning_rate': 1.7263271646629745e-05, 'epoch': 1.65}\n",
      "{'loss': 0.087, 'learning_rate': 1.6419019316493314e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0848, 'learning_rate': 1.5574766986356883e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0892, 'learning_rate': 1.4730514656220452e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0865, 'learning_rate': 1.3886262326084021e-05, 'epoch': 1.72}\n",
      "{'loss': 0.085, 'learning_rate': 1.304200999594759e-05, 'epoch': 1.74}\n",
      "{'loss': 0.0878, 'learning_rate': 1.2197757665811157e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0872, 'learning_rate': 1.1353505335674728e-05, 'epoch': 1.77}\n",
      "{'loss': 0.0897, 'learning_rate': 1.0509253005538295e-05, 'epoch': 1.79}\n",
      "{'loss': 0.0878, 'learning_rate': 9.665000675401864e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0814, 'learning_rate': 8.820748345265433e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0832, 'learning_rate': 7.976496015129003e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0853, 'learning_rate': 7.132243684992571e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0864, 'learning_rate': 6.287991354856141e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0826, 'learning_rate': 5.443739024719709e-06, 'epoch': 1.89}\n",
      "{'loss': 0.087, 'learning_rate': 4.599486694583277e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0916, 'learning_rate': 3.755234364446846e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0884, 'learning_rate': 2.9109820343104147e-06, 'epoch': 1.94}\n",
      "{'loss': 0.0882, 'learning_rate': 2.0667297041739838e-06, 'epoch': 1.96}\n",
      "{'loss': 0.0859, 'learning_rate': 1.2224773740375524e-06, 'epoch': 1.98}\n",
      "{'loss': 0.0887, 'learning_rate': 3.7822504390112117e-07, 'epoch': 1.99}\n",
      "{'train_runtime': 15463.5382, 'train_samples_per_second': 11.49, 'train_steps_per_second': 3.83, 'train_loss': 0.09847026152754416, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"t5-wikismall-dir\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=0.0001,\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_wikismall,\n",
    "    eval_dataset=tokenized_valid_wikismall,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    " \n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf73570b8194759ba24100ec2aa0d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11668 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.822, 'learning_rate': 9.571477545423381e-05, 'epoch': 0.09}\n",
      "{'loss': 2.4894, 'learning_rate': 9.14295509084676e-05, 'epoch': 0.17}\n",
      "{'loss': 2.3404, 'learning_rate': 8.714432636270141e-05, 'epoch': 0.26}\n",
      "{'loss': 2.2888, 'learning_rate': 8.285910181693521e-05, 'epoch': 0.34}\n",
      "{'loss': 2.2376, 'learning_rate': 7.857387727116902e-05, 'epoch': 0.43}\n",
      "{'loss': 2.2113, 'learning_rate': 7.428865272540281e-05, 'epoch': 0.51}\n",
      "{'loss': 2.1929, 'learning_rate': 7.000342817963662e-05, 'epoch': 0.6}\n",
      "{'loss': 2.1284, 'learning_rate': 6.571820363387042e-05, 'epoch': 0.69}\n",
      "{'loss': 2.1065, 'learning_rate': 6.143297908810422e-05, 'epoch': 0.77}\n",
      "{'loss': 2.1391, 'learning_rate': 5.7147754542338014e-05, 'epoch': 0.86}\n",
      "{'loss': 2.1292, 'learning_rate': 5.286252999657182e-05, 'epoch': 0.94}\n",
      "{'loss': 2.0801, 'learning_rate': 4.8577305450805626e-05, 'epoch': 1.03}\n",
      "{'loss': 2.0066, 'learning_rate': 4.4292080905039425e-05, 'epoch': 1.11}\n",
      "{'loss': 2.0525, 'learning_rate': 4.0006856359273224e-05, 'epoch': 1.2}\n",
      "{'loss': 1.9845, 'learning_rate': 3.572163181350703e-05, 'epoch': 1.29}\n",
      "{'loss': 2.0007, 'learning_rate': 3.143640726774083e-05, 'epoch': 1.37}\n",
      "{'loss': 2.0279, 'learning_rate': 2.7151182721974634e-05, 'epoch': 1.46}\n",
      "{'loss': 1.9819, 'learning_rate': 2.2865958176208436e-05, 'epoch': 1.54}\n",
      "{'loss': 2.0201, 'learning_rate': 1.8580733630442235e-05, 'epoch': 1.63}\n",
      "{'loss': 1.9884, 'learning_rate': 1.4295509084676037e-05, 'epoch': 1.71}\n",
      "{'loss': 1.9875, 'learning_rate': 1.001028453890984e-05, 'epoch': 1.8}\n",
      "{'loss': 1.9668, 'learning_rate': 5.725059993143641e-06, 'epoch': 1.89}\n",
      "{'loss': 1.9847, 'learning_rate': 1.4398354473774427e-06, 'epoch': 1.97}\n",
      "{'train_runtime': 3155.9486, 'train_samples_per_second': 11.09, 'train_steps_per_second': 3.697, 'train_loss': 2.1349966463096184, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"t5-wikismall-mimic-dir\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=0.0001,\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_mimic_train,\n",
    "    eval_dataset=tokenized_mimic_val,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    " \n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Preprocess the text\n",
    "    inputs = tokenizer.encode(\n",
    "        \"summarize: \" + text,\n",
    "        return_tensors='pt',\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True\n",
    "    )\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    len1 = len(inputs[0])\n",
    " \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        exponential_decay_length_penalty=((int) (len1 * 0.8), -1.05),\n",
    "        encoder_repetition_penalty=0.3,\n",
    "        no_repeat_ngram_size=4,\n",
    "        max_length=50,\n",
    "        num_beams=5,\n",
    "        temperature=0.9,\n",
    "    )\n",
    " \n",
    "    # Decode and return the summary\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A diabetes is a chronic disease that occurs when the pancreas does not produce enough insulin or when the body can not effectively use the insulin it produces. A chronic disease is a type of diabetes that occurs\n",
      "Insulin is an hormone that regulates blood glucose.\n",
      "Hyperglycaemia, also called raised blood glucose or elevated blood sugar, is a common effect of uncontrolled diabetes and over time leads to serious damage to many of the body's systems, especially the nerves and blood vessels.\n"
     ]
    }
   ],
   "source": [
    "model = model.to(DEVICE)\n",
    "with open(\"metrics/sample.txt\", 'r') as f:\n",
    "    sample = [l.strip(\"\\n\") for l in f.readlines()]\n",
    "# print(sample)\n",
    "summary = [summarize_text(l) for l in sample]\n",
    "for s in summary:\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
